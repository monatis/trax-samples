{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "trax_speech_commands.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNkj3489biie4DpH1omz0cq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/monatis/trax-samples/blob/main/trax_speech_commands.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-K8hb7bVDTO"
      },
      "source": [
        "# Audio Classification with Speech Commands Dataset in Trax\r\n",
        "By [M. Yusuf Sarıgöz](https://github.com/monatis), Google Developer Expert on Machine Learning\r\n",
        "\r\n",
        "## Introduction\r\n",
        "[Trax](https://github.com/google/trax) is an end-to-end deep learning library focusing on clear code and speed. I loved its simple yet powerful API with minimum boilerplate. You can are new to Trax, you may want to check [official examples](https://github.com/google/trax/tree/master/trax/examples) and [API docs with tutorials](https://trax-ml.readthedocs.io/en/latest/) first. Most of the official examples in either image or textual domains, so I want to provide an example in the audio domain.\r\n",
        "\r\n",
        "## Overview\r\n",
        "This is roughly a re-implementation of [Simple Audio Recognition: Recognizing Keywords](https://www.tensorflow.org/tutorials/audio/simple_audio) TensorFlow tutorial, and you may want to refer to it whenever you think a concept or code snippet is not explained.\r\n",
        "\r\n",
        "## Setup\r\n",
        "First, we will install `trax` and `pydub` libraries. `pydub` will be used to decode audio later on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVwpolEpEYQk"
      },
      "source": [
        "!pip install -U trax pydub"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1ha0Y6XZ_lp"
      },
      "source": [
        "## TPU Initialization\r\n",
        "Any Trax code can be accelerated on either GPU or TPU. To achieve this, Trax can rely on `tensorflow-numpy` or `jax`, which is the default. We can connect to and initialize TPU as the accelerator of `jax` by running this cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xJs98IwElfG"
      },
      "source": [
        "# Run this cell to set TPU in Colab\r\n",
        "import os\r\n",
        "import jax\r\n",
        "import requests\r\n",
        "# Run this to get the TPU address.\r\n",
        "if 'TPU_DRIVER_MODE' not in globals():\r\n",
        "  url = 'http://' + os.environ['COLAB_TPU_ADDR'].split(':')[0] + ':8475/requestversion/tpu_driver0.1-dev20191206'\r\n",
        "  resp = requests.post(url)\r\n",
        "  TPU_DRIVER_MODE = 1\r\n",
        "\r\n",
        "# The following is required to use TPU Driver as JAX's backend.\r\n",
        "from jax.config import config\r\n",
        "config.FLAGS.jax_xla_backend = \"tpu_driver\"\r\n",
        "config.FLAGS.jax_backend_target = \"grpc://\" + os.environ['COLAB_TPU_ADDR']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YR7nHTsta-zy"
      },
      "source": [
        "## Imports and Hyperparameters\r\n",
        "In the  following cells, we will import required modules from the `trax`package and then set some hyperparameters like `batch_size` and `num_epochs`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVOzmBxJE3iE"
      },
      "source": [
        "import trax\r\n",
        "from trax import fastmath\r\n",
        "from trax.fastmath import numpy as jnp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMFOVkizH6ic"
      },
      "source": [
        "batch_size = 256 # we can use a large batch as it runs on a TPU V2-8\r\n",
        "input_size = 32 # we will extract STFT features from audio signal to and then resize features matrix to this size\r\n",
        "num_epochs = 10\r\n",
        "num_items = 85515 # number of train samples in the Speech Commands dataset.\r\n",
        "num_steps = int(num_items // batch_size * num_epochs) # training loop will run this many steps\r\n",
        "output_dir = '/content/output' # trainer will save model checkpoints to this directory\r\n",
        "\r\n",
        "# remove output directory if you restart the notebook\r\n",
        "!rm -rf {output_dir}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9w16n4AdATw"
      },
      "source": [
        "## Data Preparation\r\n",
        "We will make use of `Serial` combinator from `trax.data` module to prepare the `Speech Commands` dataset as inputs to our model, and this function will extract STFT features from raw audio signals. Any function in a `trax.data.combinators.Serial` stack is supposed to accept a Python generator, apply zero or more transformations each one of the items from that generator and yield those transformations as a generator again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWM_Az8_KeZ0"
      },
      "source": [
        "import librosa\r\n",
        "import numpy as np\r\n",
        "import cv2\r\n",
        "def stft(gen):\r\n",
        "  it = iter(gen)\r\n",
        "  for item in it:\r\n",
        "    wave = np.asfortranarray(item[0], dtype=np.float32)\r\n",
        "    wave = np.pad(wave, 16000-item[0].shape[0])[:16000]\r\n",
        "    feat = np.abs(librosa.stft(wave, n_fft=255, hop_length=128))\r\n",
        "    feat = cv2.resize(feat, (input_size, input_size))\r\n",
        "    feat = feat.reshape(input_size, input_size, 1)\r\n",
        "    yield (feat, item[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeuJjdp_giUi"
      },
      "source": [
        "And, we will create train and evaluation dataset pipelines with the our function above. Be aware that it will download a large dataset from `tensorflow-datasets` (yes, you have access to a large collection of datasets by using `trax.data.TFDS`!)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73ny4FmPFz7h"
      },
      "source": [
        "train_ds = trax.data.Serial(\r\n",
        "    trax.data.TFDS('speech_commands', 'commands', keys=['audio', 'label'], train=True), # load train dataset from tensorflow-datasets\r\n",
        "    trax.data.Shuffle(512), # shuffle dataset in a buffer sized 512\r\n",
        "    stft, # extract stft features\r\n",
        "    trax.data.inputs.Batch(batch_size) # batch dataset to feed into our model\r\n",
        ")\r\n",
        "\r\n",
        "eval_ds = trax.data.Serial(\r\n",
        "    trax.data.TFDS('speech_commands', 'commands', keys=['audio', 'label'], train=False), # load eval dataset from tensorflow-datasets\r\n",
        "    stft, # extract features \r\n",
        "    trax.data.inputs.Batch(batch_size) # batch dataset\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3GntxN6iEtm"
      },
      "source": [
        "## Model Building\r\n",
        "We will build our model with `Serial` combinator. You have other combinator options for this, but it will be the subject of another sample notebook. The model architecture is basically two convolutional layers with 32 and 64 filters, followed by a fully connected layer with 128 features. Relu is selected as a non-linear activation, and two dropout layers are introduced to reduce overfitting. Finally, a fully-connected output layer is used with 12 neurons (number of classes in the dataset)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwOV91HorVLs"
      },
      "source": [
        "from trax.layers import tl\r\n",
        "model = tl.combinators.Serial(\r\n",
        "    tl.BatchNorm(),\r\n",
        "    tl.Conv(32, (3, 3)),\r\n",
        "    tl.Relu(),\r\n",
        "    tl.Conv(64, (3, 3)),\r\n",
        "    tl.Relu(),\r\n",
        "    tl.MaxPool(),\r\n",
        "    tl.Dropout(0.1),\r\n",
        "    tl.Flatten(),\r\n",
        "    tl.Dense(128),\r\n",
        "    tl.Dropout(0.2),\r\n",
        "    tl.Dense(12)\r\n",
        ")\r\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39oRrHymlTWI"
      },
      "source": [
        "## Training\r\n",
        "Trax allows us to define tasks for training and evaluation and provides a loop mechanism to run these tasks in the specified number. It also checkpoints the model in an output directory and logs metrics to TensorBoard under the same directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ub4aoDYW00iT"
      },
      "source": [
        "from trax.supervised import training\r\n",
        "train_task = training.TrainTask(\r\n",
        "    labeled_data=train_ds(), # training dataset we created in the data preparation section\r\n",
        "    loss_layer=tl.CategoryCrossEntropy(), # yes, losses are ordinary layers\r\n",
        "    optimizer=trax.optimizers.Adam(0.01), # we use Adam optimizer with a learning rate of 0.01\r\n",
        "    n_steps_per_checkpoint=200 # save checkpoints and log metrics every 200 steps\r\n",
        ")\r\n",
        "\r\n",
        "eval_task = training.EvalTask(\r\n",
        "    labeled_data=eval_ds(), # evaluation dataset\r\n",
        "    metrics=[tl.CategoryCrossEntropy()], # evaluation metrics. you can also add tl.Accuracy() to this list\r\n",
        "    n_eval_batches=20 # evaluate for 20 steps\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_R_pSRYmlS_"
      },
      "source": [
        "Finally, create the loop and run it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5Pwt9TIKklh"
      },
      "source": [
        "training_loop = training.Loop(\r\n",
        "    model,\r\n",
        "    train_task,\r\n",
        "    eval_tasks=[eval_task],\r\n",
        "    output_dir=output_dir # save checkpoints and TensorBoard logs in this directory\r\n",
        ")\r\n",
        "# run the loop. the first step might be slower, but subsequent ones will be much faster\r\n",
        "training_loop.run(num_steps) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFQupjXrnOMx"
      },
      "source": [
        "Congrats! Training is complete, and the model was saved in `output_dir`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6B5mKenFnhws"
      },
      "source": [
        "!ls -l {output_dir}"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}